{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c32873df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "os.sys.path.append('../utils') #To load master_configuration.yaml\n",
    "from readYAML import read_config_file #To load in master configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "239a9fab",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''TODO for Jeff: integrate convenience functions like this into the readYAML.py'''\n",
    "\n",
    "def load_configuration(file_path):\n",
    "    \n",
    "    #Read yoloConf field of yaml file\n",
    "    conf = read_config_file(file_path)['yoloConf']\n",
    "    \n",
    "    #Raise error if there's noise because there isn't supposed to be for this tutorial :-)\n",
    "    if conf['noise'] == False:\n",
    "        raise ValueError(\"Please change the 'noise' field to True in master_configuration.yaml to run this tutorial\")\n",
    "    #Check the Ultralytics fields in master_configuration.yaml based on if we're using linear or log scale images\n",
    "    else:\n",
    "        if conf['log_scale'] == True:\n",
    "            Ultconf = read_config_file(file_path)['log_scale_noise']\n",
    "        else:\n",
    "            Ultconf = read_config_file(file_path)['linear_scale_noise']\n",
    "    #Append the correct ultralytics training information to our yolo configuration\n",
    "    conf.update(Ultconf)\n",
    "    return conf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "89cd6f83",
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = load_configuration('../master_configuration.yaml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5de90b81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'use_pretrained': True,\n",
       " 'noise': True,\n",
       " 'log_scale': False,\n",
       " 'cameraX': 2048,\n",
       " 'cameraY': 1152,\n",
       " 'outputX': 512,\n",
       " 'outputY': 288,\n",
       " 'numKeyPoints': 7,\n",
       " 'GPU': True,\n",
       " 'project': '../models/noise_linear',\n",
       " 'yoloConfigFile': '../configs/keypoint_noise.yaml',\n",
       " 'suffix': '_noise'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f141e81",
   "metadata": {},
   "source": [
    "### Let's generate our keypoints as we did in part 1. I'll do this more succinctly this time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "46a46355",
   "metadata": {},
   "outputs": [],
   "source": [
    "class initial_data_processing:\n",
    "    def __init__(self,datafile):\n",
    "        self.datafile = datafile #path to simulated ER dataframe\n",
    "        self.data = self.load_data()\n",
    "        self.data = self.initial_data_transforms()\n",
    "        \n",
    "    def load_data(self):\n",
    "        df = pd.read_feather(self.datafile)\n",
    "        return df\n",
    "    \n",
    "    def initial_data_transforms(self):\n",
    "        def find_head(df,i):\n",
    "            tmp = df.iloc[i]\n",
    "            indices = pd.Series(tmp['q']).nlargest(3).index.to_numpy()\n",
    "            return np.median(tmp['x'][indices]),np.median(tmp['y'][indices])\n",
    "        xs = []\n",
    "        ys = []\n",
    "        print('Adding head points to data')\n",
    "        for i in tqdm(range(0,len(self.data))):\n",
    "            x,y = find_head(self.data,i)\n",
    "            xs.append(x)\n",
    "            ys.append(y)\n",
    "        self.data['xhead'] = xs\n",
    "        self.data['yhead'] = ys\n",
    "        self.data['htlength'] = np.sqrt((self.data['xvtx']-self.data['xhead'])**2+(self.data['yvtx']-self.data['yhead'])**2)\n",
    "        self.data['xvtx'] = self.data['xvtx']-self.data['x'].apply(lambda x: x.min())+1024\n",
    "        self.data['yvtx'] = self.data['yvtx']-self.data['y'].apply(lambda x: x.min())+1152//2\n",
    "        self.data['xhead'] = self.data['xhead']-self.data['x'].apply(lambda x: x.min())+1024\n",
    "        self.data['yhead'] = self.data['yhead']-self.data['y'].apply(lambda x: x.min())+1152//2\n",
    "\n",
    "        self.data['x'] = self.data['x'].apply(lambda x: x-x.min()+1024)\n",
    "        self.data['y'] = self.data['y'].apply(lambda x: x-x.min()+1152//2)\n",
    "        return self.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "95f25ce9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding head points to data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10500/10500 [00:04<00:00, 2100.72it/s]\n"
     ]
    }
   ],
   "source": [
    "ERs = initial_data_processing('../data/raw_simulation/ERs_cont_spectrum_correctE.feather').data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21ef3ea7",
   "metadata": {},
   "source": [
    "# Generate keypoints like in part 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c4af1ef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.sparse import coo_matrix\n",
    "\n",
    "class generate_keypoints:\n",
    "    '''Class takes a sparse image, rotates it so the head and tail are vertically aligned.\n",
    "    Then it partitions the interval between the head and tail into n_outputs + 2 equally spaced subdivisions\n",
    "    and records the (x',y') coordinate of either (a) the max intensity [if the mode parameter is set to 'max']\n",
    "    or (b), the median (x',y') over the 9 most intense pixels in each partition. The code then rotates the set of\n",
    "    (x',y')s back to the original image orientation, which are our keypoints!'''\n",
    "    \n",
    "    def __init__(self,df,i,n_outputs=1,dim=(2048,1152),mode='max'):\n",
    "        if mode.lower() != 'max' and mode.lower() != 'median':\n",
    "            raise ValueError(\"mode must be 'max' or 'median'\")\n",
    "        self.mode = mode.lower()\n",
    "        evt = df.iloc[i]\n",
    "        self.n_outputs = n_outputs\n",
    "        self.center_x = dim[1] // 2\n",
    "        self.center_y = dim[0] // 2\n",
    "        self.col = evt['x'].astype('int')\n",
    "        self.row = evt['y'].astype('int')\n",
    "        self.data = evt['q']\n",
    "        self.head = np.array([evt['xhead'],evt['yhead']])\n",
    "        self.tail = np.array([evt['xvtx'],evt['yvtx']])\n",
    "        self.rotation_angle = self.get_rotation_angle()\n",
    "        \n",
    "        '''Rotation matrices, backward rotation is used to translate track segments back to original'''\n",
    "        self.forward_rotation = self.generate_rotation_matrix(self.rotation_angle)\n",
    "        self.reverse_rotation = np.linalg.inv(self.forward_rotation)\n",
    "        \n",
    "        '''Rotate head and tail to rotated space'''\n",
    "        self.rothead = self.rotate_coord(self.head[::-1],self.forward_rotation)\n",
    "        self.rottail = self.rotate_coord(self.tail[::-1],self.forward_rotation)\n",
    "        \n",
    "        #print(self.rotate_coord(self.rothead,self.reverse_rotation))\n",
    "        '''Generate rotated sparse image'''\n",
    "        self.rot_im = self.rotate_sparse_image()\n",
    "        \n",
    "        '''Get segmented coordinates in rotated space'''\n",
    "        self.rot_segments = np.array(self.get_segment_coordinates()).T\n",
    "        self.segments = []\n",
    "        self.segments.append((evt['xvtx'],evt['yvtx']))\n",
    "        for coord in self.rot_segments:\n",
    "            self.segments.append(self.rotate_coord(coord,self.reverse_rotation)[::-1])\n",
    "        self.segments.append((evt['xhead'],evt['yhead']))\n",
    "    def get_rotation_angle(self):\n",
    "        vec = np.array([self.head[0]-self.tail[0],self.head[1]-self.tail[1]])\n",
    "        theta = np.arctan2(vec[1],vec[0])\n",
    "        return theta\n",
    "\n",
    "    def generate_rotation_matrix(self,theta):\n",
    "        cos_angle = np.cos(theta)\n",
    "        sin_angle = np.sin(theta)\n",
    "        \n",
    "        rotation_matrix = np.array([\n",
    "            [cos_angle, sin_angle],\n",
    "            [-sin_angle, cos_angle]\n",
    "        ])\n",
    "            \n",
    "        return rotation_matrix\n",
    "\n",
    "    def rotate_sparse_image(self):\n",
    "        sparse_matrix = coo_matrix((self.data, (self.row, self.col)), shape=(1152,2048))\n",
    "        # Center of the image\n",
    "\n",
    "        # Translate coordinates to origin\n",
    "        translated_x = self.col - self.center_x\n",
    "        translated_y = self.row - self.center_y\n",
    "\n",
    "        # Apply rotation\n",
    "        new_coords = np.dot(self.forward_rotation, np.array([translated_x, translated_y]))\n",
    "\n",
    "        new_x = np.round(new_coords[0] + self.center_x).astype('int')\n",
    "        new_y = np.round(new_coords[1] + self.center_y).astype('int')\n",
    "\n",
    "        # Filter out-of-bounds coordinates\n",
    "        valid_mask = (new_x >= 0) & (new_x < sparse_matrix.shape[1]) & (new_y >= 0) & (new_y < sparse_matrix.shape[0])\n",
    "        new_x = new_x[valid_mask]\n",
    "        new_y = new_y[valid_mask]\n",
    "        new_data = self.data[valid_mask]\n",
    "\n",
    "        # Create the rotated sparse matrix\n",
    "        rotated_sparse_matrix = coo_matrix((new_data, (new_y, new_x)), shape=sparse_matrix.shape)\n",
    "        return rotated_sparse_matrix.toarray().T\n",
    "    \n",
    "    def rotate_coord(self,coord,rot):\n",
    "        original_coordinate = coord\n",
    "\n",
    "        # Translate coordinate to origin\n",
    "        translated_x = original_coordinate[1] - self.center_x\n",
    "        translated_y = original_coordinate[0] - self.center_y\n",
    "\n",
    "        # Apply rotation\n",
    "        new_coord = np.dot(rot, np.array([translated_x, translated_y]))\n",
    "\n",
    "        # Translate back to the original coordinate system\n",
    "        new_coordinate = (new_coord[1] + self.center_y, new_coord[0] + self.center_x)\n",
    "        return new_coordinate\n",
    "    \n",
    "    def get_segment_coordinates(self):\n",
    "        n_partitions = self.n_outputs+2\n",
    "        y_segments = np.linspace(self.rottail[1],self.rothead[1],n_partitions)[1:-1]\n",
    "        x_segments = []\n",
    "        for seg in y_segments:\n",
    "            if self.mode == 'max':\n",
    "                x_coord = np.median(np.where(self.rot_im[int(np.round(seg)),:] == self.rot_im[int(np.round(seg)),:].max())[0])\n",
    "            elif self.mode == 'median':\n",
    "                indices = pd.Series(self.rot_im[int(np.round(seg)),:]).nlargest(9).index.to_numpy()\n",
    "                x_coord = np.median(indices)\n",
    "            x_segments.append(x_coord)\n",
    "        x_segments = np.array(x_segments)\n",
    "        if np.mean(x_segments) != 575.5 and np.mean(x_segments) != 4:\n",
    "            return x_segments,y_segments\n",
    "        else:\n",
    "            raise ValueError(\"Bad rotation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cae47e5c",
   "metadata": {},
   "source": [
    "### Let's add keypoints and YOLO information more succinctly than before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fb733294",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_keypoints(df, cut='200 > htlength > 50'):\n",
    "    df = df.query(cut)\n",
    "    df.index = [i for i in range(0,len(df))]\n",
    "\n",
    "    good_indices = []\n",
    "    coords = {} #dictionary filled with keypoint tuples\n",
    "    for i in range(0,conf['numKeyPoints']):\n",
    "        coords[i] = []\n",
    "\n",
    "    for i in tqdm(range(0,len(df))):\n",
    "        try:\n",
    "            a = generate_keypoints(df,i,n_outputs=conf['numKeyPoints']-2,mode='median')\n",
    "            good_indices.append(i)\n",
    "            for j in range(0,len(a.segments)):\n",
    "                coords[j].append(a.segments[j])\n",
    "        except:\n",
    "            #print(\"Bad rotation\")\n",
    "            continue\n",
    "    \n",
    "    '''Reduce our dataframe to only include entries where the trajectory generated'''\n",
    "    df = df.loc[df.index.isin(good_indices)] #only keep the events where the loop above didnt fail\n",
    "    df.index = [i for i in range(0,len(df))]\n",
    "    \n",
    "    '''Insert relevant YOLO columns including keypoints'''\n",
    "\n",
    "    df['class_index'] = 0\n",
    "    df['xBB'] = df['x'].apply(lambda x: (x.max()+x.min())/2 / conf['cameraX']) #normalized as a fraction of width of image (2048 pixels)\n",
    "    df['yBB'] = df['y'].apply(lambda x: (x.max()+x.min())/2 / conf['cameraY']) #normalized as a fraction of height of image (1152 pixels)\n",
    "    df['width'] = df['x'].apply(lambda x: (x.max()-x.min()) / conf['cameraX'])\n",
    "    df['height'] = df['y'].apply(lambda x: (x.max()-x.min()) / conf['cameraY'])\n",
    "\n",
    "    '''Puts key point tuples into into columns p0 to pN'''\n",
    "    for key in coords.keys():\n",
    "        df['p%s'%(key)] = coords[key]\n",
    "\n",
    "    '''Expands the tuples to p0x, p0y, p1x, p1y, etc.'''\n",
    "    # Initialize an empty dictionary to hold the new columns\n",
    "    new_columns = {}\n",
    "\n",
    "    # Iterate over each of the keypoint columns in the DataFrame\n",
    "    for col in df.columns[int(-1*conf['numKeyPoints']):]: #apologies that this is\n",
    "        # Extract x and y components from each column\n",
    "        df[[f'{col}x', f'{col}y']] = pd.DataFrame(df[col].tolist(), index=df.index)\n",
    "        # Drop the original column\n",
    "        df.drop(columns=[col], inplace=True)\n",
    "        \n",
    "    #Normalize keypoints\n",
    "    for i in range(0,conf['numKeyPoints']):\n",
    "        df['p%sx'%(i)] = df['p%sx'%(i)]/conf['cameraX']\n",
    "        df['p%sy'%(i)] = df['p%sy'%(i)]/conf['cameraY']\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "634ccd0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1118/1118 [00:03<00:00, 355.83it/s]\n"
     ]
    }
   ],
   "source": [
    "ERs = add_keypoints(ERs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffe36765",
   "metadata": {},
   "source": [
    "# Now onto something new: Making realistic simulation\n",
    "We can add dark frames to simulation to create noisy images that are more representative of real data. When adding noise, however, there are a couple of effects that need to be simulated:\n",
    "(1) Gain. We need to scale the light yield of the track relative to noise to match data. Figuring this out is a project in and of itself and something we've already done, so I'll use the scaling I use for Migdal simulation here.\n",
    "(2) Vignetting: Vignetting is a known effec tin CMOS cameras where the pixel intensity decreases radially outward from the optical center. This is another effect that we've already simulated so I'll include that here as well. Vignetting is a position dependent effect, so we should randomize track locations before applying vignetting.\n",
    "\n",
    "**Generally speaking, randomizing the locations of tracks along the readout is a good idea, as it makes trained ML models generalize better. If your model is only trained on identifying objects near the center of the readout, it will become *very* good at doing that, and only that. If a model is trained on tracks randomly scattered across the entire readout plane, then it will learn how to look for tracks regardless of position**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8037b037",
   "metadata": {},
   "outputs": [],
   "source": [
    "def determine_random_shift(i,border = 50):\n",
    "    tmp = ERs.iloc[i]\n",
    "    #Determine track boundaries\n",
    "    xmin = tmp['x'].min()\n",
    "    xmax = tmp['x'].max()\n",
    "    ymin = tmp['y'].min()\n",
    "    ymax = tmp['y'].max()\n",
    "    \n",
    "    #Perform random uniform shifts in x and y\n",
    "    xshift = np.random.randint(-1*xmin+border,2048-xmax-border)\n",
    "    yshift = np.random.randint(-1*ymin+border,1152-ymax-border)\n",
    "\n",
    "    return xshift, yshift\n",
    "\n",
    "'''Put the shift values in the dataframe so we can then apply them to other columns\n",
    "in the dataframe, thereby shifting the tracks'''\n",
    "xshifts = []\n",
    "yshifts = []\n",
    "for i in range(0,len(ERs)):\n",
    "    xshift,yshift = determine_random_shift(i,border = 50)\n",
    "    xshifts.append(xshift)\n",
    "    yshifts.append(yshift)\n",
    "ERs['xshift'], ERs['yshift'] = xshifts, yshifts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5cddafa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Now lets apply these shifts to each column'''\n",
    "xdim = 2048\n",
    "ydim = 1152\n",
    "\n",
    "'''Columns that are normalized (width and height stay the same)'''\n",
    "xcols = ['xBB','p0x', 'p1x', 'p2x', 'p3x', 'p4x', 'p5x','p6x']\n",
    "ycols = ['yBB','p0y', 'p1y', 'p2y', 'p3y', 'p4y', 'p5y','p6y']\n",
    "\n",
    "'''Unnormalize, shift, and then renormalize'''\n",
    "for col in xcols:\n",
    "    ERs[col] = (ERs[col]*xdim+ERs['xshift'])/xdim\n",
    "\n",
    "for col in ycols:\n",
    "    ERs[col] = (ERs[col]*ydim+ERs['yshift'])/ydim\n",
    "    \n",
    "'''Columns that are not normalized'''\n",
    "xcols = ['xvtx','xhead','x']\n",
    "ycols = ['yvtx','yhead','y']\n",
    "\n",
    "for col in xcols:\n",
    "    ERs[col] = (ERs[col]+ERs['xshift'])\n",
    "\n",
    "for col in ycols:\n",
    "    ERs[col] = (ERs[col]+ERs['yshift']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "dda4f0b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1118/1118 [00:00<00:00, 30979.02it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1118/1118 [00:00<00:00, 47747.48it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1118/1118 [00:00<00:00, 48020.81it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1118/1118 [00:00<00:00, 50270.50it/s]\n"
     ]
    }
   ],
   "source": [
    "'''Apply intensity scaling to match gains representative of data'''\n",
    "tqdm.pandas()\n",
    "\n",
    "'''These gain scaling factors were empirically determined'''\n",
    "res_fact = 0.115\n",
    "gf = 5\n",
    "\n",
    "def calc_light_fraction(dist,QE,f=25,N=0.85,reflect=False):\n",
    "    L = 0.5*(1 - np.sqrt( 1 - (f/(2*N*dist))*(f/(2*N*dist)) ))\n",
    "    if reflect:\n",
    "        L += 0.67*0.67*0.33*L*(dist/(dist + 2 + 2*0.57))**2\n",
    "    return L*QE*0.34\n",
    "\n",
    "light_frac = calc_light_fraction(118.7,0.23, reflect = True)\n",
    "\n",
    "def scale_evt(evt,gain_factor,light_fraction):\n",
    "    return evt*light_fraction*gain_factor/0.11\n",
    "\n",
    "def apply_gain_scaling(df,res_fact, gf):\n",
    "    df['scaled_q'] = df['q'].progress_apply(lambda x: scale_evt(x*np.random.normal(1,res_fact),gain_factor=gf,light_fraction=light_frac))\n",
    "    df['scaled_q'] = df['scaled_q'].apply(lambda x: np.round(x).astype('int16'))\n",
    "    df['idx'] = df['scaled_q'].apply(lambda x: np.where(x > 0)[0])\n",
    "    df['scaled_qsum'] = df['scaled_q'].apply(lambda x: x.sum())\n",
    "\n",
    "    df['x'] = [df['x'].iloc[i][df['idx'].iloc[i]].astype('int16') for i in tqdm(range(0,len(df)))]\n",
    "    df['y'] = [df['y'].iloc[i][df['idx'].iloc[i]].astype('int16') for i in tqdm(range(0,len(df)))]\n",
    "    df['scaled_q'] = [df['scaled_q'].iloc[i][df['idx'].iloc[i]].astype('int16') for i in tqdm(range(0,len(df)))]\n",
    "\n",
    "    \n",
    "apply_gain_scaling(ERs,gf=gf,res_fact=res_fact)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "df9ed043",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Simulate vignetting'''\n",
    "\n",
    "def apply_vignetting(df,a):\n",
    "    '''Define centroids. We apply vignetting radially outward'''\n",
    "    centroidx = 1023\n",
    "    centroidy = 575\n",
    "    \n",
    "    df['x'] = df['x'].apply(lambda x: x.astype('float32'))\n",
    "    df['y'] = df['y'].apply(lambda x: x.astype('float32'))\n",
    "    \n",
    "    '''Compute distance from centroids'''\n",
    "    df['dist_x'] = df['x'].apply(lambda x: (x-centroidx)*80/2048)\n",
    "    df['dist_y'] = df['y'].apply(lambda x: (x-centroidy)*80/2048)\n",
    "    df['dist'] = [np.sqrt(df['dist_x'].iloc[i]**2+df['dist_y'].iloc[i]**2) for i in range(0,len(df))]\n",
    "    \n",
    "    '''Apply intensity suppression due to vigentting'''\n",
    "    df['vignetted_q'] = [df['scaled_q'].iloc[i]/(a**2/(a-df['dist'].iloc[i])**2) for i in range(0,len(df))]\n",
    "    df['vignetted_q'] = df['vignetted_q'].apply(lambda x: np.round(x).astype('int16'))\n",
    "    del(df['dist_x'])\n",
    "    del(df['dist_y'])\n",
    "    del(df['dist'])\n",
    "    \n",
    "    ''' This code removes all charge that's 0 after correcting for vignetting and turning into an integer '''\n",
    "    df['x'] = df['x'].apply(lambda x: x.astype('int16'))\n",
    "    df['y'] = df['y'].apply(lambda x: x.astype('int16'))\n",
    "    \n",
    "    df['idx'] = df['vignetted_q'].apply(lambda x: np.where(x > 0)[0])\n",
    "    df['x'] = [df['x'].iloc[i][df['idx'].iloc[i]].astype('int16') for i in tqdm(range(0,len(df)))]\n",
    "    df['y'] = [df['y'].iloc[i][df['idx'].iloc[i]].astype('int16') for i in tqdm(range(0,len(df)))]\n",
    "    df['vignetted_q'] = [df['vignetted_q'].iloc[i][df['idx'].iloc[i]].astype('int16') for i in tqdm(range(0,len(df)))]\n",
    "\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b0a93bf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1118/1118 [00:00<00:00, 53797.80it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1118/1118 [00:00<00:00, 53732.46it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1118/1118 [00:00<00:00, 54033.37it/s]\n"
     ]
    }
   ],
   "source": [
    "ERs = apply_vignetting(ERs,a=95) #a is also empirically determined"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9e7c3be",
   "metadata": {},
   "source": [
    "### Now we've applied both gain and vignetting scalings to the intensities. The next step is to add noise. Rather than simulating noise, it's best to use dark frames recorded by MIGDAL. Dark frames are those where the camera captures images with the rest of the MIGDAL TPC powered down. The noise distribution varies so we pick dark frames at random from a sample of 200 (we could pick more but loading dense 2048 x 1152 arrays can fill up system memory pretty quickly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "607f87f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage import io\n",
    "dark = io.imread('../darks/MIG_Dark_0V_230803T130339.DARK.0001.MTIFF',plugin='pil')\n",
    "'''Load masterdark, we will subtract this from the dark frames'''\n",
    "masterdark = np.load('../darks/master_dark_230803.npz')['arr_0']\n",
    "dark = dark - masterdark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8e5035a",
   "metadata": {},
   "source": [
    "### Let's downsample our dark frames using 4x4 binning. These aren't sparse arrays, so we have to do this differently than what we've been doing previously. Pytorch's AvgPool function is one way to do this. I prefer using it because it can be computed on a GPU which is much faster than on a CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ccf77f69",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "ap = nn.AvgPool2d(kernel_size = (4,4),stride = (4,4),divisor_override = 1)\n",
    "darkDownSample = ap(torch.tensor(dark)).numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab0af481",
   "metadata": {},
   "source": [
    "### Remake our training, validation, and test sets with the shifted data. The noise will only be added to images for YOLO to evaluate (we should add it to our dataframe but we don't need to yet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d6271bbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set size: 782\n",
      "val set size : 224\n",
      "test set size: 112\n",
      "sum : 1118\n",
      "dataset size: 1118\n"
     ]
    }
   ],
   "source": [
    "'''I\"m manually splitting the data up into 70% train, 20% validation, 10% test. There are better and more\n",
    "statistically robust ways of doing this, like using k-fold cross validation which I linked an article on\n",
    "above. You also ALWAYS want to shuffle your data before splitting it up. A lot of scikit-learn\"s convenience\n",
    "functions automatically shuffle for you but we\"ll do it manually here'''\n",
    "\n",
    "dataset_size = len(ERs)\n",
    "# Shuffle data\n",
    "ERs = ERs.sample(frac=1,random_state=42) #Random state ensures we get identical shuffles every time for reproducability\n",
    "ERs.index = [i for i in range(0,len(ERs))] #reset index after shuffling\n",
    "ERs['index'] = ERs.index\n",
    "data = {} #dictionary storing train, validation, and test datasets\n",
    "data['train'] = ERs[:int(dataset_size*0.7)]\n",
    "data['valid'] = ERs[int(dataset_size*0.7):int(dataset_size*0.9)]\n",
    "data['test'] =  ERs[int(dataset_size*0.9):]\n",
    "print('Train set size: %s\\nval set size : %s\\ntest set size: %s\\nsum : %s\\ndataset size: %s'%(len(data['train']),len(data['valid']),len(data['test']),len(data['train'])+len(data['valid'])+len(data['test']),dataset_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac6c3035",
   "metadata": {},
   "source": [
    "### Now we generate noisy pngs. The image processing code is a tiny bit different than before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "91a05400",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.image\n",
    "def save_images(settype):\n",
    "    \n",
    "    if settype.lower() != 'train' and settype.lower() != 'test' and settype.lower() !='valid':\n",
    "        raise ValueError(\"settype must be 'train','valid',or 'test'\")\n",
    "    \n",
    "    path = '../datasets/%s%s/images'%(settype.lower(),conf['suffix'])\n",
    "    \n",
    "    #Create our output directory if it doesn't already exist\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "    \n",
    "    for i in tqdm(range(0,len(data[settype.lower()]))):\n",
    "        \n",
    "        tmp = data[settype.lower()].iloc[i]\n",
    "        \n",
    "        '''Setting bins to (512,288) downsamples the image with 4x4 binning'''\n",
    "        im = np.histogram2d(tmp['x'],tmp['y'],weights=tmp['vignetted_q'],bins=(512,288),range=((0,2048),(0,1152)))[0].T\n",
    "        \n",
    "        '''Add noise'''\n",
    "        dark_idx = np.random.randint(0,200)\n",
    "        im += darkDownSample[dark_idx]\n",
    "        \n",
    "        '''The colorscale (vmin and vmax) as well as how we define im depend on if we use linear or logarithmic\n",
    "        colorscale images'''\n",
    "        if conf['log_scale'] == False:\n",
    "            matplotlib.image.imsave('%s/%s.png'%(path,tmp['index']), im, vmin=-100, vmax=600,cmap = 'jet')\n",
    "        else:\n",
    "            im[im<0] = 0 #We probably shouldn't do this but this allows for clean log scale images\n",
    "            im = np.log10(im+1)\n",
    "            matplotlib.image.imsave('%s/%s.png'%(path,tmp['index']), im, vmin=1.4, vmax=2.5,cmap = 'jet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f6da1931",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:43<00:00, 17.95it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 224/224 [00:12<00:00, 17.88it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 112/112 [00:06<00:00, 18.00it/s]\n"
     ]
    }
   ],
   "source": [
    "for key in ['train','valid','test']:\n",
    "    save_images(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4f8a405f",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Ditto here: whether or not we use a log scale is determined from master_configuration.yaml'''\n",
    "def save_labels(settype):\n",
    "    if settype.lower() != 'train' and settype.lower() != 'test' and settype.lower() !='valid':\n",
    "        raise ValueError(\"settype must be 'train','valid',or 'test'\")\n",
    "\n",
    "    path = '../datasets/%s%s/labels/'%(settype.lower(),conf['suffix'])\n",
    "    \n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "\n",
    "    for i in range(0,len(data[settype.lower()])):\n",
    "        tmp = data[settype.lower()].iloc[i]\n",
    "        series = tmp[['class_index','xBB', 'yBB', 'width','height', 'p0x', 'p0y', 'p1x', 'p1y', 'p2x', 'p2y', 'p3x', 'p3y', 'p4x','p4y', 'p5x', 'p5y', 'p6x', 'p6y']]\n",
    "        with open(path+'%s.txt'%(tmp['index']), 'w') as f:\n",
    "            series_str = ' '.join(map(str, series.values))\n",
    "            f.write(series_str + '\\n')\n",
    "            f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c3c81b47",
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in ['train','valid']:\n",
    "    save_labels(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "522fd72d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Noisy data files. The track shifts are randomized so we save separate sets for linear and log colorscale\n",
    "for key in ['train','valid','test']:\n",
    "    data[key].index = [i for i in range(0,len(data[key]))]\n",
    "    data[key].to_feather(\"../data/%s%s.feather\"%(key,conf['suffix']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adf34571",
   "metadata": {},
   "source": [
    "### Now we can finally train on our realistic noisy sim and then compare the keypoints to truth in the part2 notebookcript running overnight, or sending your data to Jeff so he can train it on a GPU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "da78454c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'use_pretrained': True,\n",
       " 'noise': True,\n",
       " 'log_scale': False,\n",
       " 'cameraX': 2048,\n",
       " 'cameraY': 1152,\n",
       " 'outputX': 512,\n",
       " 'outputY': 288,\n",
       " 'numKeyPoints': 7,\n",
       " 'GPU': True,\n",
       " 'project': '../models/noise_linear',\n",
       " 'yoloConfigFile': '../configs/keypoint_noise.yaml',\n",
       " 'suffix': '_noise'}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29273885",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New https://pypi.org/project/ultralytics/8.2.18 available ðŸ˜ƒ Update with 'pip install -U ultralytics'\n",
      "Ultralytics YOLOv8.1.8 ðŸš€ Python-3.10.10 torch-2.0.0+cu117 CUDA:0 (NVIDIA GeForce RTX 3090, 24257MiB)\n",
      "\u001b[34m\u001b[1mengine/trainer: \u001b[0mtask=pose, mode=train, model=yolov8m-pose.yaml, data=../configs/keypoint_noise.yaml, epochs=1000, time=None, patience=25, batch=16, imgsz=512, save=True, save_period=-1, cache=False, device=None, workers=8, project=../models/noise_linear, name=train, exist_ok=False, pretrained=True, optimizer=auto, verbose=True, seed=0, deterministic=True, single_cls=False, rect=True, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, multi_scale=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, vid_stride=1, stream_buffer=False, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, embed=None, show=False, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=False, opset=None, workspace=4, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, label_smoothing=0.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0, auto_augment=randaugment, erasing=0.4, crop_fraction=1.0, cfg=None, tracker=botsort.yaml, save_dir=../models/noise_linear/train\n",
      "Overriding model.yaml kpt_shape=[17, 3] with kpt_shape=[7, 2]\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1      1392  ultralytics.nn.modules.conv.Conv             [3, 48, 3, 2]                 \n",
      "  1                  -1  1     41664  ultralytics.nn.modules.conv.Conv             [48, 96, 3, 2]                \n",
      "  2                  -1  2    111360  ultralytics.nn.modules.block.C2f             [96, 96, 2, True]             \n",
      "  3                  -1  1    166272  ultralytics.nn.modules.conv.Conv             [96, 192, 3, 2]               \n",
      "  4                  -1  4    813312  ultralytics.nn.modules.block.C2f             [192, 192, 4, True]           \n",
      "  5                  -1  1    664320  ultralytics.nn.modules.conv.Conv             [192, 384, 3, 2]              \n",
      "  6                  -1  4   3248640  ultralytics.nn.modules.block.C2f             [384, 384, 4, True]           \n",
      "  7                  -1  1   1991808  ultralytics.nn.modules.conv.Conv             [384, 576, 3, 2]              \n",
      "  8                  -1  2   3985920  ultralytics.nn.modules.block.C2f             [576, 576, 2, True]           \n",
      "  9                  -1  1    831168  ultralytics.nn.modules.block.SPPF            [576, 576, 5]                 \n",
      " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 12                  -1  2   1993728  ultralytics.nn.modules.block.C2f             [960, 384, 2]                 \n",
      " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 15                  -1  2    517632  ultralytics.nn.modules.block.C2f             [576, 192, 2]                 \n",
      " 16                  -1  1    332160  ultralytics.nn.modules.conv.Conv             [192, 192, 3, 2]              \n",
      " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 18                  -1  2   1846272  ultralytics.nn.modules.block.C2f             [576, 384, 2]                 \n",
      " 19                  -1  1   1327872  ultralytics.nn.modules.conv.Conv             [384, 384, 3, 2]              \n",
      " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 21                  -1  2   4207104  ultralytics.nn.modules.block.C2f             [960, 576, 2]                 \n",
      " 22        [15, 18, 21]  1   4338781  ultralytics.nn.modules.head.Pose             [1, [7, 2], [192, 384, 576]]  \n",
      "YOLOv8m-pose summary: 320 layers, 26419405 parameters, 26419389 gradients, 81.2 GFLOPs\n",
      "\n",
      "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir ../models/noise_linear/train', view at http://localhost:6006/\n",
      "Freezing layer 'model.22.dfl.conv.weight'\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks with YOLOv8n...\n",
      "Downloading https://github.com/ultralytics/assets/releases/download/v8.1.0/yolov8n.pt to 'yolov8n.pt'...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6.23M/6.23M [00:00<00:00, 10.3MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed âœ…\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /home/jeef/workspace/Migdal_backup/keyPoint_tutorial/datasets/train_noise/labels... 782 images, 0 backgrounds, 5 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [00:00<00:00, 2259.92it/s]\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING âš ï¸ /home/jeef/workspace/Migdal_backup/keyPoint_tutorial/datasets/train_noise/images/291.png: ignoring corrupt image/label: negative label values [  -0.045298    -0.10403   -0.042019    -0.11134    -0.03874    -0.11866]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING âš ï¸ /home/jeef/workspace/Migdal_backup/keyPoint_tutorial/datasets/train_noise/images/444.png: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0049      1.0125      1.0201      1.0277]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING âš ï¸ /home/jeef/workspace/Migdal_backup/keyPoint_tutorial/datasets/train_noise/images/497.png: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0951      1.1035      1.1119      1.1202]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING âš ï¸ /home/jeef/workspace/Migdal_backup/keyPoint_tutorial/datasets/train_noise/images/500.png: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0616      1.0738      1.0859      1.0981]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING âš ï¸ /home/jeef/workspace/Migdal_backup/keyPoint_tutorial/datasets/train_noise/images/730.png: ignoring corrupt image/label: negative label values [   -0.10499    -0.10083   -0.096667]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: /home/jeef/workspace/Migdal_backup/keyPoint_tutorial/datasets/train_noise/labels.cache\n",
      "WARNING âš ï¸ No 'flip_idx' array defined in data.yaml, setting augmentation 'fliplr=0.0'\n",
      "WARNING âš ï¸ 'rect=True' is incompatible with DataLoader shuffle, setting shuffle=False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /home/jeef/workspace/Migdal_backup/keyPoint_tutorial/datasets/valid_noise/labels... 224 images, 0 backgrounds, 1 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 224/224 [00:00<00:00, 1822.11it/s]\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING âš ï¸ /home/jeef/workspace/Migdal_backup/keyPoint_tutorial/datasets/valid_noise/images/911.png: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0834      1.0813]\n",
      "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: /home/jeef/workspace/Migdal_backup/keyPoint_tutorial/datasets/valid_noise/labels.cache\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plotting labels to ../models/noise_linear/train/labels.jpg... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m SGD(lr=0.01, momentum=0.9) with parameter groups 83 weight(decay=0.0), 93 weight(decay=0.0005), 92 bias(decay=0.0)\n",
      "\u001b[34m\u001b[1mTensorBoard: \u001b[0mmodel graph visualization added âœ…\n",
      "Image sizes 512 train, 512 val\n",
      "Using 8 dataloader workers\n",
      "Logging results to \u001b[1m../models/noise_linear/train\u001b[0m\n",
      "Starting training for 1000 epochs...\n",
      "\n",
      "      Epoch    GPU_mem   box_loss  pose_loss  kobj_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     1/1000      2.78G      6.682      7.392          0      11.62      4.349          7        512: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 49/49 [00:05<00:00,  9.55it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Pose(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:00<00:00,  7.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        223        223          0          0          0          0          0          0          0          0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss  pose_loss  kobj_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     2/1000      2.74G      6.633      6.503          0      9.543      4.132          7        512: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 49/49 [00:04<00:00, 11.88it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Pose(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:00<00:00,  8.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        223        223          0          0          0          0          0          0          0          0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss  pose_loss  kobj_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     3/1000      2.76G       5.74      5.883          0      5.122      3.555          8        512: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 49/49 [00:04<00:00, 12.07it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Pose(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:00<00:00,  8.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        223        223    0.00259     0.0628      0.026    0.00313     0.0128      0.309      0.156     0.0591\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss  pose_loss  kobj_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     4/1000      2.75G      3.036      4.127          0      2.003      2.206          7        512: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 49/49 [00:04<00:00, 12.06it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Pose(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:00<00:00,  8.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        223        223      0.613      0.589      0.577      0.183      0.442      0.484      0.357      0.114\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss  pose_loss  kobj_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     5/1000      2.75G       2.63      3.141          0      1.414      1.845          8        512: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 49/49 [00:03<00:00, 12.26it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Pose(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:00<00:00,  8.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        223        223      0.471      0.789       0.57      0.217      0.442       0.72      0.498      0.189\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss  pose_loss  kobj_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     6/1000      2.74G      2.341       2.52          0      1.283      1.622          8        512: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 49/49 [00:04<00:00, 12.23it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Pose(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:00<00:00,  8.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        223        223       0.87      0.903      0.894       0.33      0.839      0.885      0.827      0.366\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss  pose_loss  kobj_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     7/1000      2.76G      2.145      2.209          0      1.136      1.491          8        512: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 49/49 [00:04<00:00, 12.09it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Pose(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:00<00:00,  8.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        223        223      0.912      0.934      0.948      0.401      0.873      0.894      0.905      0.477\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss  pose_loss  kobj_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     8/1000      2.76G      2.016      1.971          0      1.027      1.414          6        512: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 49/49 [00:04<00:00, 12.22it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Pose(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:00<00:00,  8.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        223        223      0.884      0.969      0.952      0.439      0.868      0.951      0.929      0.547\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss  pose_loss  kobj_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     9/1000      2.75G      1.975      1.832          0      1.006      1.368          9        512: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 49/49 [00:04<00:00, 12.03it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Pose(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:00<00:00,  8.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        223        223      0.906      0.964      0.969      0.405      0.883      0.933      0.949      0.631\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss  pose_loss  kobj_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "    10/1000      2.75G      1.889      1.788          0     0.9391      1.325          9        512: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 49/49 [00:04<00:00, 12.22it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Pose(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:00<00:00,  7.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        223        223      0.952      0.968      0.985      0.509      0.938      0.955      0.961      0.612\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss  pose_loss  kobj_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "    11/1000      2.77G      1.811      1.608          0     0.9229      1.276          9        512: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 49/49 [00:04<00:00, 12.10it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Pose(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:00<00:00,  8.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        223        223      0.987      0.986      0.994      0.499      0.956      0.951      0.948       0.65\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss  pose_loss  kobj_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "    12/1000      2.76G      1.773      1.552          0     0.8591      1.248          9        512: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 49/49 [00:04<00:00, 12.20it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Pose(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:00<00:00,  8.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        223        223      0.955      0.942      0.976       0.48       0.95      0.937      0.966      0.597\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from ultralytics import YOLO\n",
    "\n",
    "# Load a base model\n",
    "model = YOLO('yolov8m-pose.yaml')  # load empty model. Can choose from yolov8{n,s,m,l,x}-pose.yaml. Letters are ordered from smallest model to largest\n",
    "\n",
    "#Function to train YOLO\n",
    "#The project field sets the directory where YOLO's trained weights will be assigned\n",
    "model.train(data=conf['yoloConfigFile'],project=conf['project'],epochs=1000,patience=25,imgsz=512,rect=True)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
