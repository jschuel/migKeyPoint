{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4ad727c4",
   "metadata": {},
   "source": [
    "### In this module we will run YOLO on our test set and analyze its outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13382cb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "from ultralytics import YOLO\n",
    "import torch\n",
    "import migKeyPoint.utils.YAMLtools as yt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18787748",
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = yt.load_configuration('../master_configuration.yaml')['yoloConf']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "051f6a7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "conf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e13ce9ca",
   "metadata": {},
   "source": [
    "### Load test set\n",
    "\n",
    "**In this document conf['project_dir'] will be the parent directory for most things**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0956a602",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Calling our testset dataframe df'''\n",
    "df = pd.read_feather(conf['project_dir']+\"/data/test%s.feather\"%(conf['suffix']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f077e91a",
   "metadata": {},
   "source": [
    "# Now let's evaluate YOLO on the test images\n",
    "\n",
    "After training yolo it will create a directory called *runs/*. You'll need to navigate through runs and find the\n",
    "desired model you trained. In the example below *train13/* is the directory that we're using. The *weights/* subdirectory contains two files, one called 'best' and the other called 'last'. 'best' is the saved weights file during the best performing epoch of training. It's typically best to use this file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01f35c98",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "'''You may need to manually set the weights path. If you do multiple trainings in the same project\n",
    "ultralytics will append version numbers to the directories where the weights are located...for instance\n",
    "train2/ train3/ train/50, etc'''\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "#Directory of the test PNG images we created is determined by conf['suffix']\n",
    "\n",
    "testpath = conf['project_dir']+'/datasets/test%s/images/'%(conf['suffix'])\n",
    "\n",
    "#Create a sorted list of PIL images to pass into YOLO\n",
    "images = [Image.open(testpath+val) for val in sorted(os.listdir(testpath))]\n",
    "print('Performing YOLO inference\\n')\n",
    "\n",
    "#Load trained YOLO model weights. Where we look will depend on if the model is pretrained or not\n",
    "if len(conf['pretrained_model_path']) != 0:\n",
    "    try:\n",
    "        weights_file = conf['pretrained_model_path']+'/best.pt'\n",
    "        model = YOLO(weights_file)\n",
    "        print(f\"Found weights file at {weights_file}\")\n",
    "    except:\n",
    "        print(f\"Weights file not found at {weights_file} trying {conf['pretrained_model_path']}\")\n",
    "        try:\n",
    "            weights_file = conf['pretrained_model_path']\n",
    "            model = YOLO(weights_file)\n",
    "            print(f\"Found weights file at {conf['pretrained_model_path']}\")\n",
    "        except:\n",
    "            raise OSError(f\"Couldn't find a model at {weights_file}. Did you mean to use a pretrained_model path?\")\n",
    "else:\n",
    "    weights_file = conf['project']+'/train/weights/best.pt'\n",
    "    model = YOLO(weights_file)\n",
    "\n",
    "#Perform YOLO inference on all images\n",
    "results = model.predict(images,batch = len(images),verbose = False, workers = 1,imgsz=512,rect=True)\n",
    "print(\"DONE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bb4d840",
   "metadata": {},
   "source": [
    "### Let's take a closer look at the results file before analyzing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e86499c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#The length of the results list should always be the same as the test dataframe\n",
    "print(len(results),len(df))\n",
    "\n",
    "#Looking through the results list, each entry has a boxes and keypoints object\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2fc13f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now lets look at boxes and keypoints for the first image\n",
    "print(results[0].boxes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a1957bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#The above shows several options for boxes. The xyxyn format gives [xmin,ymin,xmax,ymax] of the bounding box\n",
    "#normalized to 1. This is what we should use so we can upscale the box to the 2048 x 1152 image\n",
    "\n",
    "results[0].boxes.xyxyn.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c6c9bee",
   "metadata": {},
   "outputs": [],
   "source": [
    "#It's possible that there can be more than one bounding box per image, let's check if any images identified more than\n",
    "#one track. If they do, any additionalbounding boxes beyond the first would be a false positive\n",
    "\n",
    "for i,res in enumerate(results):\n",
    "    if len(res.boxes.xyxyn) > 1:\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71b0b8e1",
   "metadata": {},
   "source": [
    "### Now lets take a look at the keypoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7daf8aec",
   "metadata": {},
   "outputs": [],
   "source": [
    "results[0].keypoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fb4e213",
   "metadata": {},
   "outputs": [],
   "source": [
    "#the .xyn field contains the normalized keypoint coordinates so lets use these so we can scale them up to 2048 x 1152\n",
    "#Let's quickly check if all 9 keypoints are found in each image\n",
    "\n",
    "for i,res in enumerate(results):\n",
    "    if len(res.keypoints.xyn[0]) != conf['maxNumKeyPoints']:\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d69747d",
   "metadata": {},
   "source": [
    "# Lets aggregate our keypoint and bounding box predictions into a dataframe so we can quantitatively compare them with truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4ba3d5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "### The columns of interest are 'class_index', 'xBB', 'yBB', 'width',\n",
    "### 'height', 'pkx', 'pky'; k \\in [0,...,maxNumKeyPoints]\n",
    "### We need to convert these back to the aspect ratio of the camera\n",
    "\n",
    "xcols = ['xBB', 'width'] + ['p%sx'%(i) for i in range(0,conf['maxNumKeyPoints'])]\n",
    "ycols = ['yBB', 'height'] + ['p%sy'%(i) for i in range(0,conf['maxNumKeyPoints'])]\n",
    "\n",
    "for col in xcols:\n",
    "    df[col] = df[col]*conf['cameraX']\n",
    "\n",
    "for col in ycols:\n",
    "    df[col] = df[col]*conf['cameraY']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "129e5a9b",
   "metadata": {},
   "source": [
    "### Our bounding box dimensions are [xmin,xmax,ymin,ymax] so lets make columns for these in our test dataframe too"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8980fc3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Bounding box perimeters'''\n",
    "df['xmin'] = np.round(df['xBB']-df['width']/2).astype('int')\n",
    "df['xmax'] = np.round(df['xBB']+df['width']/2).astype('int')\n",
    "\n",
    "df['ymin'] = np.round(df['yBB']-df['height']/2).astype('int')\n",
    "df['ymax'] = np.round(df['yBB']+df['height']/2).astype('int')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bfe4639",
   "metadata": {},
   "source": [
    "### Now let's compile the YOLO results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1032f6e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''We compile results into track-indexed data but will store the frame index so we can link back\n",
    "to data indexed by image frame. Since we only had one track per frame in this example, there will be a one-to-one\n",
    "correspondence between frame-indexed and track-indexed data. That will not be the case in general. \n",
    "The code below handles the general case.'''\n",
    "\n",
    "yolo = pd.DataFrame() #YOLO results dataframe\n",
    "xmin = []\n",
    "xmax = []\n",
    "ymin = []\n",
    "ymax = []\n",
    "pred = [] #class prediction, should be 0 (ER) for each event in this sample\n",
    "prob = [] #class prediction confidence score between 0 and 1. 1 is most confident, 0 means the model has no idea\n",
    "frameIndex = []\n",
    "coords = {}\n",
    "\n",
    "#Fill coords dictionary with lists of each keypoint\n",
    "for i in range(0,conf['maxNumKeyPoints']):\n",
    "    coords[i] = []\n",
    "for i,res in enumerate(results):\n",
    "    boxes = res.boxes.xyxyn.cpu().numpy() #tensor of all of the boxes converted to 4 x Nboxes numpy array\n",
    "    points = res.keypoints.xyn.cpu().numpy() #tensor of all sets of key points as (1 x 2) x NkeyPointSets array \n",
    "    data = res.boxes.data.cpu().numpy() #to get class prediction and class confidence score\n",
    "    for datum, box, point in zip(data,boxes,points): #loop through all boxes and sets of key points in frame i\n",
    "        frameIndex.append(i) #frame index\n",
    "        xmin.append(box[0]*conf['cameraX'])\n",
    "        ymin.append(box[1]*conf['cameraY'])\n",
    "        xmax.append(box[2]*conf['cameraX'])\n",
    "        ymax.append(box[3]*conf['cameraY'])\n",
    "        pred.append(datum[5])\n",
    "        prob.append(datum[4])\n",
    "        #grab each of the N key points to put into the coords[j] list these are still normalized\n",
    "        for j,p in enumerate(point):\n",
    "            coords[j].append(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3e5ce54",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Now we populate the yolo results dataframe'''\n",
    "yolo['frame'] = frameIndex #frame number\n",
    "yolo['xmin'] = xmin\n",
    "yolo['xmax'] = xmax\n",
    "yolo['ymin'] = ymin\n",
    "yolo['ymax'] = ymax\n",
    "yolo['pred'] = pred\n",
    "yolo['prob'] = prob\n",
    "for i in range(0,conf['maxNumKeyPoints']):\n",
    "    yolo['p%s'%(i)] = coords[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1dfaf11",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Now lets change the pj\"s to pjx and pjy and also convert to resolution of the camera'''\n",
    "# Initialize an empty dictionary to hold the new columns\n",
    "new_columns = {}\n",
    "\n",
    "# Iterate over each of the keypoint columns in the DataFrame\n",
    "for col in yolo.columns[int(-1*conf['maxNumKeyPoints']):]:\n",
    "    # Extract x and y components from each column\n",
    "    yolo[[f'{col}x', f'{col}y']] = pd.DataFrame(yolo[col].tolist(), index=yolo.index)\n",
    "    # Drop the original column\n",
    "    yolo.drop(columns=[col], inplace=True)\n",
    "    \n",
    "'''Scale pjx and pjy to aspect ratio of images'''\n",
    "for i in range(0,conf['maxNumKeyPoints']):\n",
    "    yolo['p%sx'%(i)] *= conf['cameraX']\n",
    "    yolo['p%sy'%(i)] *= conf['cameraY']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9e9144c",
   "metadata": {},
   "source": [
    "### Let's compare YOLO's output to our original test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32250a1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test set (truth)\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d73fca1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test set (truth)\n",
    "len(yolo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "673dc68c",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''From the two cells above we see that YOLO predicted two more tracks than there actually are so we need to\n",
    "handle these properly. On real data we won\"t know how many tracks there actually are per frame, so a good way to\n",
    "handle this is to group YOLO\"s output by frame number (this is the index of our truth test set) and then aggregate\n",
    "the contents of YOLO\"s output'''\n",
    "\n",
    "grp = yolo.groupby('frame').agg(list).reset_index()\n",
    "grp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7313f54",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''grp is the output of grouping YOLO\"s output and then aggregating. You can see that the difference between\n",
    "grp and yolo is that everything is now in a list and the length of grp now matches the length of our\n",
    "truth test set (df). It\"s important to note that if we had any false negatives, the length of grp would be \n",
    "shorter than the length of our test set. This is because each frame in the test set has one track track.'''\n",
    "\n",
    "#Now let's check the frames with false positives\n",
    "\n",
    "'''This statement grabs all events where the length of the list of predictions is greater than 1, i.e.\n",
    "predictions with two tracks when we know in actuality each frame had one track. Selecting ['prob'] we \n",
    "analyze the confidences in each track'''\n",
    "grp[grp['pred'].apply(lambda x: len(x) > 1)]['prob']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e5bc430",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Here are the confidences of all tracks. The two track events have lower confidences in their predictions\n",
    "than average'''\n",
    "plt.hist(yolo['prob'],bins=51);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c4bcebd",
   "metadata": {},
   "source": [
    "### Let's plot an example output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ce8d9ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Assumes test dataframe is called df and yolo results are called yolo\n",
    "def plot_output(i,zoom): #zoom zooms to truth frame\n",
    "    tmp = df.iloc[i]\n",
    "    tmpyolo = grp.query('frame == %s'%(i))\n",
    "    '''Use vignetted_q with noise, otherwise q'''\n",
    "    try:\n",
    "        im = np.histogram2d(tmp['x'],tmp['y'],weights=tmp['q'],bins=(2048,1152),range=((0,2048),(0,1152)))[0].T\n",
    "    except:\n",
    "        im = np.histogram2d(tmp['x'],tmp['y'],weights=tmp['vignetted_q'],bins=(2048,1152),range=((0,2048),(0,1152)))[0].T\n",
    "    plt.imshow(im,cmap='viridis')\n",
    "    \n",
    "    #Truth bounding box in white\n",
    "    plt.hlines(tmp['ymin'],tmp['xmin'],tmp['xmax'],color='w',lw=2)\n",
    "    plt.hlines(tmp['ymax'],tmp['xmin'],tmp['xmax'],color='w',lw=2)\n",
    "    plt.vlines(tmp['xmin'],tmp['ymin'],tmp['ymax'],color='w',lw=2)\n",
    "    plt.vlines(tmp['xmax'],tmp['ymin'],tmp['ymax'],color='w',lw=2)\n",
    "    \n",
    "    for xmin,xmax,ymin,ymax in zip(tmpyolo['xmin'],tmpyolo['xmax'],tmpyolo['ymin'],tmpyolo['ymax']):\n",
    "        #Predicted bounding boxes in cyan\n",
    "        plt.hlines(ymin,xmin,xmax,color='cyan',lw=2)\n",
    "        plt.hlines(ymax,xmin,xmax,color='cyan',lw=2)\n",
    "        plt.vlines(xmin,ymin,ymax,color='cyan',lw=2)\n",
    "        plt.vlines(xmax,ymin,ymax,color='cyan',lw=2)\n",
    "    \n",
    "    if zoom:\n",
    "        plt.xlim(tmp['xmin']-15,tmp['xmax']+15)\n",
    "        plt.ylim(tmp['ymin']-15,tmp['ymax']+15)\n",
    "    \n",
    "    #Truth keypoints in white, predicted keypoints in cyan\n",
    "    for i in range(0,conf['maxNumKeyPoints']):\n",
    "        if i == 0:\n",
    "            plt.plot(tmp['p%sx'%(i)],tmp['p%sy'%(i)],'o',color='k')\n",
    "            for x,y in zip(tmpyolo['p%sx'%(i)],tmpyolo['p%sy'%(i)]):\n",
    "                plt.plot(x,y,'o',color='magenta')\n",
    "        else:\n",
    "            plt.plot(tmp['p%sx'%(i)],tmp['p%sy'%(i)],'o',color='w')\n",
    "            for x,y in zip(tmpyolo['p%sx'%(i)],tmpyolo['p%sy'%(i)]):\n",
    "                plt.plot(x,y,'o',color='cyan')\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "278194db",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Recall when assessing the results that our model was trained on noisy image. The images shown here\n",
    "are the truth ERs without noise (I didn't save the noise in the processing)'''\n",
    "\n",
    "plot_output(18,zoom = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1f02a01",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Here\"s one with two tracks'''\n",
    "plot_output(73,zoom = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbe661e7",
   "metadata": {},
   "source": [
    "# Let's quantify our performance\n",
    "\n",
    "We'll start with bounding box quantification. We use the IOU which stands for \"intersection over union\" metric. This metric is the ratio of the areas of the intersection to union of the truth and predicted bounding boxes. Perfect overlap is 1, no overlap is 0\n",
    "\n",
    "![](../../figures/IOU_def.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b530a6d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bb_intersection_over_union(xmin1, xmax1, ymin1, ymax1, xmin2, xmax2, ymin2, ymax2):\n",
    "    \n",
    "    # determine the (x, y)-coordinates of the intersection rectangle\n",
    "    xA = max(xmin1, xmin2)\n",
    "    yA = max(ymin1, ymin2)\n",
    "    xB = min(xmax1, xmax2)\n",
    "    yB = min(ymax1, ymax2)\n",
    "    \n",
    "    # compute the area of intersection rectangle\n",
    "    interArea = max(0, xB - xA + 1) * max(0, yB - yA + 1)\n",
    "    \n",
    "    # compute the area of both the prediction and ground-truth rectangles\n",
    "    \n",
    "    boxAArea = (xmax1 - xmin1 + 1) * (ymax1 - ymin1 + 1)\n",
    "    boxBArea = (xmax2 - xmin2 + 1) * (ymax2 - ymin2 + 1)\n",
    "    \n",
    "    # compute the intersection over union by taking the intersection\n",
    "    # area and dividing it by the sum of prediction + ground-truth\n",
    "    # areas - the interesection area so we don't double count the intersection\n",
    "    \n",
    "    iou = interArea / (boxAArea + boxBArea - interArea)\n",
    "\n",
    "    return iou"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "711ba860",
   "metadata": {},
   "outputs": [],
   "source": [
    "IOUs = []\n",
    "for i in range(0,len(df)):\n",
    "    tmp = df.iloc[i]\n",
    "    tmpyolo = yolo.query('frame == %s'%(i))\n",
    "    if len(tmpyolo) != 1:\n",
    "        IOU = -1\n",
    "    else:\n",
    "        tmpyolo = tmpyolo.iloc[0]\n",
    "        IOU = bb_intersection_over_union(tmp['xmin'], tmp['xmax'], tmp['ymin'], tmp['ymax'], \n",
    "                                     tmpyolo['xmin'], tmpyolo['xmax'], tmpyolo['ymin'], tmpyolo['ymax'])\n",
    "    IOUs.append(IOU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6661dc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = pd.DataFrame()\n",
    "metrics['IOU'] = IOUs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11bd7475",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's visualize our IOU scores\n",
    "\n",
    "plt.hist(metrics['IOU'],range=(0,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3f27c0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Need to add IOU to grp. We'll just remake grp to do this...in an analysis with much larger datasets you want\n",
    "### to be more careful about when to do large operations, but here we have very small datasets, so it's okay\n",
    "\n",
    "grp = yolo.groupby('frame').agg(list).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f597c0d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generally speaking our IOUs look great. Lets see what the best and worst overlapping events look like\n",
    "\n",
    "worstIOUidx = metrics['IOU'].nsmallest(1).index.to_numpy()[0]\n",
    "bestIOUidx = metrics['IOU'].nlargest(1).index.to_numpy()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e05fd4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_output(worstIOUidx,zoom = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae32f673",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_output(bestIOUidx,zoom = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1292288c",
   "metadata": {},
   "source": [
    "# Next lets assess our keypoint detection performance\n",
    "Coming up with a good metric to holistically assess keypoint assignments is important, and there's a good deal of flexibilty of how to come up with one. Genreally speaking, with ML metrics, we want something that ranges from 0 to 1 with 0 being maximally bad and 1 being maximally good. The computer vision community often uses a metric called [object keypoint similarity](https://learnopencv.com/object-keypoint-similarity/), which satisfies these criteria. We'll use a simplified version of object keypoint similarity and begin by defining the keypoint similarity of point $i$ as\n",
    "$$\n",
    "KS_{i} = \\exp\\left(\\frac{-d_i^2}{2s^2k^2}\\right),\n",
    "$$\n",
    "\n",
    "where $d_i$ is the Euclidean distance between point the truth and predicted keypoint $i$, $s$ is a scale parameter defined as the area of the truth bounding box, and $k$ is an empirically determined constant. For an image we'll compute the object keypoint similarity, $\\rm OKS$, as\n",
    "$$\n",
    "\\mathrm{OKS}=\\frac{1}{N_\\mathrm{keypoints}}\\sum_{i=1}^{N_\\mathrm{keypoints}}KS_i\n",
    "$$\n",
    "\n",
    "**Important, OKS is an ordered quantity. The order of the keypoints determined by YOLO is supposed to match the truth ordering. For applications where direction matters, we want OKS to be order dependent. If we don't care about direction, we can reorder the points before computing OKS in such a way that it minimizes the Euclidean distance.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1da4d61",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Define a function for order-dependents OKS between truth and measured'''\n",
    "def order_dependent_OKS(truth,meas,k): #arguments are two tracks which we can think of as rows of dataframes\n",
    "    distances = []\n",
    "    for i in range(0,conf['maxNumKeyPoints']): #Loop through each point in the event\n",
    "        d = np.sqrt((meas['p%sx'%(i)]-truth['p%sx'%(i)])**2+(meas['p%sy'%(i)]-truth['p%sy'%(i)])**2)\n",
    "        distances.append(d)\n",
    "    scale = truth['width']*truth['height'] #This is s, we'll compute truth bounding box relative to image size for this\n",
    "    OKS = np.exp(-1*(np.array(distances)**2)/(2*scale**2*k**2)).sum()/conf['maxNumKeyPoints']\n",
    "    print(OKS)\n",
    "    return OKS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f57d01b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Compute OKS'''\n",
    "OKSs = []\n",
    "for i in range(0,len(df)):\n",
    "    tmp = df.iloc[i]\n",
    "    tmpyolo = yolo.query('frame == %s'%(i))\n",
    "    if len(tmpyolo) != 1:\n",
    "        OKS = -1\n",
    "    else:\n",
    "        tmpyolo = tmpyolo.iloc[0]\n",
    "        OKS = order_dependent_OKS(tmp,tmpyolo,k=0.001)\n",
    "    OKSs.append(OKS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be4d297a",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Add OKS to the metrics dataframe'''\n",
    "metrics['OKS'] = OKSs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d449209",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(metrics['OKS'],range=(0,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4da6c82b",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics.query('OKS > 0')['OKS'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe4f8572",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's see what the best and worst keypoint images look like\n",
    "\n",
    "worstOKSidx = metrics.query('OKS > 0')['OKS'].nsmallest(2).index.to_numpy()[1]\n",
    "avgOKSidxs = metrics.query('0.9 > OKS > 0.88')['OKS'].nsmallest(10).index.to_numpy()\n",
    "bestOKSidx = metrics['OKS'].nlargest(1).index.to_numpy()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1108aee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_output(worstOKSidx,zoom = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6ef12ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_output(avgOKSidxs[0],zoom = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31132481",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_output(bestOKSidx,zoom = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a156f83c",
   "metadata": {},
   "source": [
    "# A few other things we can do:\n",
    "1. Repeat this exercise for images with multiple tracks (great way to play around with more realistic data sets!)\n",
    "2. Compute a head/tail OKS score only considering head and tail points. Then you can compare the OKS with the order YOLO gave versus the reverse order. This would be more interesting in a more diverse set where we get head/tail wrong sometimes\n",
    "3. Assess how good the heads and tails individually were assessed\n",
    "4. Repeat these notebooks with more realistic simulation that includes Noise (Done)\n",
    "5. Play around with estimating track angles based on keypoint trajectories and comparing to truth\n",
    "\n",
    "### We now have a starting point for labeling data. We can use our pretrained model and test how it works when labeling data in Label Studio"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
