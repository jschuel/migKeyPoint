{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c32873df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import migKeyPoint.utils.YAMLtools as yt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "692f1548",
   "metadata": {},
   "source": [
    "### Aside: let's begin by setting up our environment by reading master_configuration.yaml\n",
    "\n",
    "Conf will be used later on when we get into generating images and labels for training YOLO. I've set up this configuration environment so the user doesn't have to find specific notebook cells to edit. Instead, you can just edit master_configuration.yaml. You can feel free to expand master_configuration.yaml as needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eeac576",
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = yt.load_configuration('../master_configuration.yaml')['yoloConf']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9bedf24",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Conf is a dictionary'''\n",
    "conf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f141e81",
   "metadata": {},
   "source": [
    "### Okay, now the processing begins. Let's start by loading simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95f25ce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Load simulated ERs'''\n",
    "ERs = pd.read_feather('../simulation/ERs_cont_spectrum_correctE.feather')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cc93d4a",
   "metadata": {},
   "source": [
    "### Now we approximate the head as the point with highest intensity and use the truth simulated vertex as the tail. These are our track endpoints and we will use these as boundaries to define our keypoint training labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bf6b681",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_head(df,i):\n",
    "    tmp = df.iloc[i]\n",
    "    indices = pd.Series(tmp['q']).nlargest(3).index.to_numpy()\n",
    "    return np.median(tmp['x'][indices]),np.median(tmp['y'][indices])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ac36592",
   "metadata": {},
   "outputs": [],
   "source": [
    "xs = []\n",
    "ys = []\n",
    "for i in tqdm(range(0,len(ERs))):\n",
    "    x,y = find_head(ERs,i)\n",
    "    xs.append(x)\n",
    "    ys.append(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32b102eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "ERs['xhead'] = xs\n",
    "ERs['yhead'] = ys"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d238433",
   "metadata": {},
   "source": [
    "### To keep \"nice\" ER tracks, we can cut on the distance between the track vertex and track head which I'm calling 'htlength' which is short for head/tail length. \n",
    "This isn't something we generally want to do, but I'm doing this so we can automatically generate decent-looking keypoint sets easily. In general we want something that works even for ERs following the most complicated trajectories, but this is just a demonstration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b45e28da",
   "metadata": {},
   "outputs": [],
   "source": [
    "ERs['htlength'] = np.sqrt((ERs['xvtx']-ERs['xhead'])**2+(ERs['yvtx']-ERs['yhead'])**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87a7db7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Whenever you define a new variable that you can make cuts on, it\"s generally good practice\n",
    "to look at the distribution of the variable'''\n",
    "\n",
    "plt.hist(ERs['htlength'],bins=51)\n",
    "plt.xlabel('Head-tail Euclidean distance [pixels]')\n",
    "plt.yscale('log')\n",
    "plt.show()\n",
    "\n",
    "'''Most tracks have short head/tail lengths. We\"re going to consider longer lengths, \n",
    "as trajectories are less ambiguous for these'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96510c62",
   "metadata": {},
   "source": [
    "### Centering tracks to make rotations easier. This step isn't strictly necessary but I'm doing it for this example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9142fbe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "conf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f780bf9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Center all tracks. First shift the vertex and head columns, then do the entire track'''\n",
    "ERs['xvtx'] = ERs['xvtx']-ERs['x'].apply(lambda x: x.min())+conf['cameraX']//2\n",
    "ERs['yvtx'] = ERs['yvtx']-ERs['y'].apply(lambda x: x.min())+conf['cameraY']//2\n",
    "ERs['xhead'] = ERs['xhead']-ERs['x'].apply(lambda x: x.min())+conf['cameraX']//2\n",
    "ERs['yhead'] = ERs['yhead']-ERs['y'].apply(lambda x: x.min())+conf['cameraY']//2\n",
    "\n",
    "ERs['x'] = ERs['x'].apply(lambda x: x-x.min()+conf['cameraX']//2)\n",
    "ERs['y'] = ERs['y'].apply(lambda x: x-x.min()+conf['cameraY']//2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21ef3ea7",
   "metadata": {},
   "source": [
    "# This method is what we're using to generate simulated key points.\n",
    "**These keypoints are what we will use to label our data to train YOLO. Any supervised learning task requires to be trained on labeled data. For this example, I thought it would be better to automate the key point generation, rather than making us label a ton of data by hand.**\n",
    "\n",
    "You might be wondering, \"wait, if we can automatically generate key points, then what's the point of using YOLO?\" Well, the point of training YOLO is to come up with a general way of identifying key points. The method we're using here works because (a) we don't have noise to begin with, and most importantly, (b) we know the vertex (tail) of simulated ER tracks. Knowledge of the vertex is needed for the method below to work and in general, the tail of ERs is difficult to find in real data using first principles approaches. So, we'll use this method to generate sets of key points to train YOLO on, then we'll train YOLO, and then we'll test it's predictions on data it wasn't trained on and see how it compares to this method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4af1ef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.sparse import coo_matrix\n",
    "\n",
    "class generate_keypoints:\n",
    "    '''Class takes a sparse image, rotates it so the head and tail are vertically aligned.\n",
    "    Then it partitions the interval between the head and tail into n_outputs + 2 equally spaced subdivisions\n",
    "    and records the (x',y') coordinate of either (a) the max intensity [if the mode parameter is set to 'max']\n",
    "    or (b), the median (x',y') over the 9 most intense pixels in each partition. The code then rotates the set of\n",
    "    (x',y')s back to the original image orientation, which are our keypoints!'''\n",
    "    \n",
    "    def __init__(self,df,i,n_outputs=1,dim=(2048,1152),mode='max'):\n",
    "        if mode.lower() != 'max' and mode.lower() != 'median':\n",
    "            raise ValueError(\"mode must be 'max' or 'median'\")\n",
    "        self.mode = mode.lower()\n",
    "        evt = df.iloc[i]\n",
    "        self.n_outputs = n_outputs\n",
    "        self.center_x = dim[1] // 2\n",
    "        self.center_y = dim[0] // 2\n",
    "        self.col = evt['x'].astype('int')\n",
    "        self.row = evt['y'].astype('int')\n",
    "        self.data = evt['q']\n",
    "        self.head = np.array([evt['xhead'],evt['yhead']])\n",
    "        self.tail = np.array([evt['xvtx'],evt['yvtx']])\n",
    "        self.rotation_angle = self.get_rotation_angle()\n",
    "        \n",
    "        '''Rotation matrices, backward rotation is used to translate track segments back to original'''\n",
    "        self.forward_rotation = self.generate_rotation_matrix(self.rotation_angle)\n",
    "        self.reverse_rotation = np.linalg.inv(self.forward_rotation)\n",
    "        \n",
    "        '''Rotate head and tail to rotated space'''\n",
    "        self.rothead = self.rotate_coord(self.head[::-1],self.forward_rotation)\n",
    "        self.rottail = self.rotate_coord(self.tail[::-1],self.forward_rotation)\n",
    "        \n",
    "        #print(self.rotate_coord(self.rothead,self.reverse_rotation))\n",
    "        '''Generate rotated sparse image'''\n",
    "        self.rot_im = self.rotate_sparse_image()\n",
    "        \n",
    "        '''Get segmented coordinates in rotated space'''\n",
    "        self.rot_segments = np.array(self.get_segment_coordinates()).T\n",
    "        self.segments = []\n",
    "        self.segments.append((evt['xvtx'],evt['yvtx']))\n",
    "        for coord in self.rot_segments:\n",
    "            self.segments.append(self.rotate_coord(coord,self.reverse_rotation)[::-1])\n",
    "        self.segments.append((evt['xhead'],evt['yhead']))\n",
    "    def get_rotation_angle(self):\n",
    "        vec = np.array([self.head[0]-self.tail[0],self.head[1]-self.tail[1]])\n",
    "        theta = np.arctan2(vec[1],vec[0])\n",
    "        return theta\n",
    "\n",
    "    def generate_rotation_matrix(self,theta):\n",
    "        cos_angle = np.cos(theta)\n",
    "        sin_angle = np.sin(theta)\n",
    "        \n",
    "        rotation_matrix = np.array([\n",
    "            [cos_angle, sin_angle],\n",
    "            [-sin_angle, cos_angle]\n",
    "        ])\n",
    "            \n",
    "        return rotation_matrix\n",
    "\n",
    "    def rotate_sparse_image(self):\n",
    "        sparse_matrix = coo_matrix((self.data, (self.row, self.col)), shape=(1152,2048))\n",
    "        # Center of the image\n",
    "\n",
    "        # Translate coordinates to origin\n",
    "        translated_x = self.col - self.center_x\n",
    "        translated_y = self.row - self.center_y\n",
    "\n",
    "        # Apply rotation\n",
    "        new_coords = np.dot(self.forward_rotation, np.array([translated_x, translated_y]))\n",
    "\n",
    "        new_x = np.round(new_coords[0] + self.center_x).astype('int')\n",
    "        new_y = np.round(new_coords[1] + self.center_y).astype('int')\n",
    "\n",
    "        # Filter out-of-bounds coordinates\n",
    "        valid_mask = (new_x >= 0) & (new_x < sparse_matrix.shape[1]) & (new_y >= 0) & (new_y < sparse_matrix.shape[0])\n",
    "        new_x = new_x[valid_mask]\n",
    "        new_y = new_y[valid_mask]\n",
    "        new_data = self.data[valid_mask]\n",
    "\n",
    "        # Create the rotated sparse matrix\n",
    "        rotated_sparse_matrix = coo_matrix((new_data, (new_y, new_x)), shape=sparse_matrix.shape)\n",
    "        return rotated_sparse_matrix.toarray().T\n",
    "    \n",
    "    def rotate_coord(self,coord,rot):\n",
    "        original_coordinate = coord\n",
    "\n",
    "        # Translate coordinate to origin\n",
    "        translated_x = original_coordinate[1] - self.center_x\n",
    "        translated_y = original_coordinate[0] - self.center_y\n",
    "\n",
    "        # Apply rotation\n",
    "        new_coord = np.dot(rot, np.array([translated_x, translated_y]))\n",
    "\n",
    "        # Translate back to the original coordinate system\n",
    "        new_coordinate = (new_coord[1] + self.center_y, new_coord[0] + self.center_x)\n",
    "        return new_coordinate\n",
    "    \n",
    "    def get_segment_coordinates(self):\n",
    "        n_partitions = self.n_outputs+2\n",
    "        y_segments = np.linspace(self.rottail[1],self.rothead[1],n_partitions)[1:-1]\n",
    "        x_segments = []\n",
    "        for seg in y_segments:\n",
    "            if self.mode == 'max':\n",
    "                x_coord = np.median(np.where(self.rot_im[int(np.round(seg)),:] == self.rot_im[int(np.round(seg)),:].max())[0])\n",
    "            elif self.mode == 'median':\n",
    "                indices = pd.Series(self.rot_im[int(np.round(seg)),:]).nlargest(9).index.to_numpy()\n",
    "                x_coord = np.median(indices)\n",
    "            x_segments.append(x_coord)\n",
    "        x_segments = np.array(x_segments)\n",
    "        if np.mean(x_segments) != 575.5 and np.mean(x_segments) != 4:\n",
    "            return x_segments,y_segments\n",
    "        else:\n",
    "            raise ValueError(\"Bad rotation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd76a307",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Let\"s test the output of this method. We\"ll run it on one event and then plot the output'''\n",
    "\n",
    "#We're generating keypoints for track 11 of the set of ERs with htlength > 11\n",
    "#Remember conf['maxNumKeyPoints'] is the total number of keypoints in our sample which includes the head and tail\n",
    "#We therefore generate conf['maxNumKeyPoints']-2 points\n",
    "#Feel free to play around with the n_outputs parameter here but make sure you set the desired number of\n",
    "#key points in master_configuration.yaml when performing a study\n",
    "a = generate_keypoints(ERs.query('htlength > 50'),i = 11,n_outputs = conf['maxNumKeyPoints']-2,mode = 'median') #can change 'median' to 'max' to test what happens\n",
    "\n",
    "'''Our simulation has track coordinates stored as sparse arrays, meaning it only saves the coordinates\n",
    "of pixels with non-zero content. To form images our of sparse arrays you can just histogram the sparse\n",
    "array content, setting the bins and range to be appropriate for the camera'''\n",
    "im = np.histogram2d(a.col,a.row,weights=a.data,bins=(2048,1152),range=((0,2048),(0,1152)))[0].T\n",
    "\n",
    "#plot image\n",
    "plt.imshow(im,cmap='jet')\n",
    "#Zoom into track\n",
    "plt.xlim(a.col.min()-5,a.col.max()+5)\n",
    "plt.ylim(a.row.min()-5,a.row.max()+5)\n",
    "\n",
    "#Plot segments we generated\n",
    "for seg in a.segments: #a.segments are the output of generate_keypoints\n",
    "    plt.plot(seg[0],seg[1],'o',color='magenta')\n",
    "\n",
    "plt.xlim()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ce76708",
   "metadata": {},
   "source": [
    "### Now let's process all events with 50 < htlength < 200. This cut gives us the longer ER tracks but omits the exceptionally long outliers we saw in the previous histogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a022963f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "'''There\"s an apparent bug in the keypoint generation code where certain kinds of rotations mess things up.\n",
    "I\"ve figured out how to flag these, but haven\"t figured out how to fix this. Nevertheless, since this is\n",
    "meant to be a minimal working example, we won\"t sweat this. Because of this bug, there will be a preferred \n",
    "direction in the trajectories that do generate. Generally speaking training on a preferred direction will \n",
    "bias a machine learning model which we don\"t want.'''\n",
    "\n",
    "ERs = ERs.query('200 > htlength > 50')\n",
    "ERs.index = [i for i in range(0,len(ERs))]\n",
    "\n",
    "good_indices = []\n",
    "coords = {} #dictionary filled with keypoint tuples\n",
    "for i in range(0,len(a.segments)):\n",
    "    coords[i] = []\n",
    "\n",
    "for i in tqdm(range(0,len(ERs))):\n",
    "    try:\n",
    "        a = generate_keypoints(ERs,i,n_outputs=conf['maxNumKeyPoints']-2,mode='median')\n",
    "        good_indices.append(i)\n",
    "        for j in range(0,len(a.segments)):\n",
    "            coords[j].append(a.segments[j])\n",
    "    except:\n",
    "        #print(\"Bad rotation\")\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6c3e34a",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Reduce our dataframe to only include entries where the trajectory generated'''\n",
    "ERs = ERs.loc[ERs.index.isin(good_indices)] #only keep the events where the loop above didnt fail\n",
    "ERs.index = [i for i in range(0,len(ERs))]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b7734d2",
   "metadata": {},
   "source": [
    "### Now we need to format our data consistent with what YOLOv8 requires. We use the format with dim = 2 from https://docs.ultralytics.com/datasets/pose/#supported-dataset-formats shown below\n",
    "\n",
    "class-index, x, y, width, height, px1, py1, px2, py2,...,pxn, pyn\n",
    "\n",
    "Where\n",
    "\n",
    "**class_index**: 0 (choice is up to us but this is what I use for ERs)\\\n",
    "**xBB**: x coordinate of the center of the bounding box (BB) surrounding the track\\\n",
    "**yBB**: y coordinate of the center of the bounding box surrounding the track\\\n",
    "**width**: width of the bounding box\\\n",
    "**height**: height of the bounding box\\\n",
    "**pxN**: x coordinate of the Nth key point. **Key points should be ordered so we'll get head/tail out of the box!** \\\n",
    "**pyN**: y coordinate of the Nth key point. **Key points should be ordered so we'll get head/tail out of the box!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7cc88ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Whenever I\"m processing data, I like to check the columns in my dataframe periodically'''\n",
    "ERs.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c17f0afd",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Make class_index, x, y, width, and height columns'''\n",
    "\n",
    "ERs['class_index'] = 0\n",
    "\n",
    "'''Columns \"x\", \"y\", and \"q\" are array-based quantities. We can use pandas\"s apply function and\n",
    "lambda expressions to do array operations rowwise in a succinct way. Don\"t worry if this notation\n",
    "seems cryptic, once you get used to it, it\"s very succinct and efficient for array operations\n",
    "in pandas dataframes'''\n",
    "\n",
    "ERs['xBB'] = ERs['x'].apply(lambda x: (x.max()+x.min())/2 / conf['cameraX']) #normalized as a fraction of width of image (2048 pixels)\n",
    "ERs['yBB'] = ERs['y'].apply(lambda x: (x.max()+x.min())/2 / conf['cameraY']) #normalized as a fraction of height of image (1152 pixels)\n",
    "ERs['width'] = ERs['x'].apply(lambda x: (x.max()-x.min()) / conf['cameraX'])\n",
    "ERs['height'] = ERs['y'].apply(lambda x: (x.max()-x.min()) / conf['cameraY'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e80cac49",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Notice the configuration variables in these expressions. Let\"s remind ourselves of the contents of\n",
    "our configuration file'''\n",
    "conf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ecba2c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Lets plot a test image with a bounding box to test that we did it right'''\n",
    "index = 15\n",
    "tmp = ERs.iloc[index] #we'll plot frame 8\n",
    "im = np.histogram2d(tmp['x'],tmp['y'],weights=tmp['q'],bins=(conf['cameraX'],conf['cameraY']),range=((0,conf['cameraX']),(0,conf['cameraY'])))[0].T\n",
    "plt.imshow(im,cmap='jet')\n",
    "plt.xlim(tmp['x'].min()-10,tmp['x'].max()+10)\n",
    "plt.ylim(tmp['y'].min()-10,tmp['y'].max()+10)\n",
    "xmin = (tmp['xBB']-tmp['width']/2)*conf['cameraX']\n",
    "xmax = (tmp['xBB']+tmp['width']/2)*conf['cameraX']\n",
    "ymin = (tmp['yBB']-tmp['height']/2)*conf['cameraY']\n",
    "ymax = (tmp['yBB']+tmp['height']/2)*conf['cameraY']\n",
    "plt.hlines(ymin,xmin,xmax,color='magenta',lw=2)\n",
    "plt.hlines(ymax,xmin,xmax,color='magenta',lw=2)\n",
    "plt.vlines(xmin,ymin,ymax,color='magenta',lw=2)\n",
    "plt.vlines(xmax,ymin,ymax,color='magenta',lw=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bf3f508",
   "metadata": {},
   "source": [
    "### A reminder in case it isn't clear: the reason why we are able to generate bounding boxes like this is because we know the true boundaries of simulated tracks! This allows us to generate all labels automatically so we can train YOLO without hand labeling data\n",
    "\n",
    "Now we will put our generated keypoints into the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05aa16f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Puts key point tuples into into columns p0 to pN'''\n",
    "for key in coords.keys():\n",
    "    ERs['p%s'%(key)] = coords[key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5239b81f",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Expands the tuples to p0x, p0y, p1x, p1y, etc.'''\n",
    "# Initialize an empty dictionary to hold the new columns\n",
    "new_columns = {}\n",
    "\n",
    "# Iterate over each of the keypoint columns in the DataFrame\n",
    "for col in ERs.columns[int(-1*conf['maxNumKeyPoints']):]: #apologies that this is\n",
    "    # Extract x and y components from each column\n",
    "    ERs[[f'{col}x', f'{col}y']] = pd.DataFrame(ERs[col].tolist(), index=ERs.index)\n",
    "    # Drop the original column\n",
    "    ERs.drop(columns=[col], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c7abea7",
   "metadata": {},
   "source": [
    "### Don't forget to normalize the keypoints too!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eeba922",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,conf['maxNumKeyPoints']):\n",
    "    ERs['p%sx'%(i)] = ERs['p%sx'%(i)]/conf['cameraX']\n",
    "    ERs['p%sy'%(i)] = ERs['p%sy'%(i)]/conf['cameraY']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e72138c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Check the columns of our dataframe'''\n",
    "ERs.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a92817c4",
   "metadata": {},
   "source": [
    "### Now let's generate image and label files for our training, validation, and test sets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e10054bf",
   "metadata": {},
   "source": [
    "### Step 0: Determine train / validation / test split. We're going to do this in the most rudimentary way. Scikit learn has a lot of information about good ways to do this that are worth looking up when you have the time. Here are a few examples:\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.StratifiedKFold.html\n",
    "\n",
    "https://machinelearningmastery.com/k-fold-cross-validation/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f048799",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''I\"m manually splitting the data up into 70% train, 20% validation, 10% test. There are better and more\n",
    "statistically robust ways of doing this, like using k-fold cross validation which I linked an article on\n",
    "above. You also ALWAYS want to shuffle your data before splitting it up. A lot of scikit-learn\"s convenience\n",
    "functions automatically shuffle for you but we\"ll do it manually here'''\n",
    "\n",
    "dataset_size = len(ERs)\n",
    "# Shuffle data\n",
    "ERs = ERs.sample(frac=1,random_state=42) #Random state ensures we get identical shuffles every time for reproducability\n",
    "ERs.index = [i for i in range(0,len(ERs))] #reset index after shuffling\n",
    "ERs['index'] = ERs.index\n",
    "data = {} #dictionary storing train, validation, and test datasets\n",
    "data['train'] = ERs[:int(dataset_size*0.7)]\n",
    "data['valid'] = ERs[int(dataset_size*0.7):int(dataset_size*0.9)]\n",
    "data['test'] =  ERs[int(dataset_size*0.9):]\n",
    "print('Train set size: %s\\nval set size : %s\\ntest set size: %s\\nsum : %s\\ndataset size: %s'%(len(data['train']),len(data['valid']),len(data['test']),len(data['train'])+len(data['valid'])+len(data['test']),dataset_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "066a62b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "conf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b75ee2f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.image\n",
    "def save_images(settype):\n",
    "    \n",
    "    if settype.lower() != 'train' and settype.lower() != 'test' and settype.lower() !='valid':\n",
    "        raise ValueError(\"settype must be 'train','valid',or 'test'\")\n",
    "    \n",
    "    path = conf['project_dir'] + '/datasets/%s%s/images'%(settype.lower(),conf['suffix'])\n",
    "    print(f\"Saving images to path: {path}\")\n",
    "    \n",
    "    #Create our output directory if it doesn't already exist\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "    \n",
    "    for i in tqdm(range(0,len(data[settype.lower()]))):\n",
    "        \n",
    "        tmp = data[settype.lower()].iloc[i]\n",
    "        \n",
    "        '''Setting bins to (512,288) downsamples the image with 4x4 binning'''\n",
    "        im = np.histogram2d(tmp['x'],tmp['y'],weights=tmp['q'],bins=(512,288),range=((0,2048),(0,1152)))[0].T\n",
    "        \n",
    "        '''The colorscale (vmin and vmax) as well as how we define im depend on if we use linear or logarithmic\n",
    "        colorscale images'''\n",
    "        if conf['log_scale'] == False:\n",
    "            matplotlib.image.imsave('%s/%s.png'%(path,tmp['index']), im, vmin=0, vmax=35000,cmap = 'jet')\n",
    "        else:\n",
    "            im = np.log10(im+1)\n",
    "            matplotlib.image.imsave('%s/%s.png'%(path,tmp['index']), im, vmin=0, vmax=4,cmap = 'jet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0b59da1",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Recall that master_configuration.yaml tells us if we\"re using a log colorscale or not. This is built into the \n",
    "save_images function so the user doesn\"t have to declare it here'''\n",
    "for key in ['train','valid','test']:\n",
    "    save_images(key)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "787d91c2",
   "metadata": {},
   "source": [
    "### Step 2: Generate labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00643729",
   "metadata": {},
   "outputs": [],
   "source": [
    "conf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47656e87",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Ditto here: whether or not we use a log scale is determined from master_configuration.yaml'''\n",
    "def save_labels(settype):\n",
    "    if settype.lower() != 'train' and settype.lower() != 'test' and settype.lower() !='valid':\n",
    "        raise ValueError(\"settype must be 'train','valid',or 'test'\")\n",
    "\n",
    "    path = conf['project_dir'] + '/datasets/%s%s/labels/'%(settype.lower(),conf['suffix'])\n",
    "    print(f\"Saving labels to path: {path}\")\n",
    "    \n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "\n",
    "    for i in range(0,len(data[settype.lower()])):\n",
    "        tmp = data[settype.lower()].iloc[i]\n",
    "        series = tmp[['class_index','xBB', 'yBB', 'width','height'] + list(np.array([['p%sx'%(i), 'p%sy'%(i)] for i in range(0,conf['maxNumKeyPoints'])]).flatten())]\n",
    "        with open(path+'%s.txt'%(tmp['index']), 'w') as f:\n",
    "            series_str = ' '.join(map(str, series.values))\n",
    "            f.write(series_str + '\\n')\n",
    "            f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6806943",
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in ['train','valid']:\n",
    "    save_labels(key)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e20f449a",
   "metadata": {},
   "source": [
    "### Step 3: Save our train, validation, and test dataframes for usage later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9451455b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Need to reset indices in order to save as feather\n",
    "for key in ['train','valid','test']:\n",
    "    data[key].index = [i for i in range(0,len(data[key]))]\n",
    "    outfile = conf['project_dir'] + \"/data/%s%s.feather\"%(key,conf['suffix'])\n",
    "    if not os.path.exists(os.path.split(outfile)[0]):\n",
    "        os.makedirs(os.path.split(outfile)[0])\n",
    "    print(f\"Saving metadata to {outfile}\")    \n",
    "    data[key].to_feather(outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adf34571",
   "metadata": {},
   "source": [
    "# Now lets train YOLO (training will take a while. If you don't have access to a GPU, I would recommend leaving this script running overnight, or sending your data to Jeff so he can train it on a GPU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5010bff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "### First we need to make a training configuration file\n",
    "#The yt module does this automatically for us\n",
    "\n",
    "yt.create_keypoint_config('../master_configuration.yaml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1042debb",
   "metadata": {},
   "outputs": [],
   "source": [
    "conf['project']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29273885",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "\n",
    "# Load a model\n",
    "model = YOLO('yolov8m-pose.yaml')  # load empty model. Can choose from yolov8{n,s,m,l,x}-pose.yaml. Letters are ordered from smallest model to largest\n",
    "        \n",
    "'''There are a lot of arguments for the train function that are not used here, so it may\n",
    "be worth looking up some of the other options in the Ultralytics documentation for the \n",
    "train() function. master_configuration.yaml should handle some of these arguments pretty smoothly\n",
    "but nevertheless, here are brief descriptions of the ones used here\n",
    "\n",
    "data: The .yaml file we generated in the cell above this\n",
    "\n",
    "epochs: Maximum number of times the training script will loop over the entire dataset. It first loops through the\n",
    "training set and then evaluates on the validation set. The validation metrics are what we use to determine\n",
    "how well the model performs as its being trained. This is because the model isn\"t trained on the validation set.\n",
    "\n",
    "imgsz: The size of the maximum width of the image. We downsampled using 4 x 4 binning, thereby reducing the\n",
    "2048 x 1152 images to 512 x 288.\n",
    "\n",
    "rect: Setting this to true tells YOLO that these aren't square images and that it needs to measure the smaller\n",
    "dimension scaled to the 512 we put in imgsz (this means it will automatically detect 288 pixels in y)\n",
    "\n",
    "patience: This is an important training parameter more commonly called \"early stopping\" in ML parlance.\n",
    "patience = 25 means that the algorithm will train until 25 successive epochs have elapsed where the model\n",
    "didn't improve in a predefined metric (here its the average precision metric). In other words, if the model\n",
    "didn\"t \"improve its learning on the validation set\" after 25 epochs then it will terminate training. This is important to avoid\n",
    "overtraining models.\n",
    "\n",
    "device: Whether we train on CPU or GPU. Leave this field empty for now\n",
    "'''\n",
    "\n",
    "#Function to train YOLO\n",
    "#The project field sets the directory where YOLO's trained weights will be assigned\n",
    "model.train(data='/home/jeef/workspace/migKeyPoint/migKeyPoint/tutorial/configs/keypoint.yaml',project=conf['project'],epochs=1000,patience=25,imgsz=512,rect=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6ebee71",
   "metadata": {},
   "source": [
    "### We've now successfully used simulation to train YOLO to be a keypoint detector. Let's move to Part 2 and evaluate it on the test set"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
